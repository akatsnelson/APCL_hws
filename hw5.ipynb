{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98604cb",
   "metadata": {},
   "source": [
    "Домашку будет легче делать в колабе (убедитесь, что у вас runtype с gpu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c422aa0",
   "metadata": {},
   "source": [
    "# Задание 1 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a72790",
   "metadata": {},
   "source": [
    "Обучите word2vec модели с негативным семплированием (cbow и skip-gram) аналогично тому, как это было сделано в семинаре. Вам нужно изменить следующие пункты: \n",
    "1) добавьте лемматизацию в предобработку (любым способом)  \n",
    "2) измените размер окна в большую или меньшую сторону\n",
    "3) измените размерность итоговых векторов\n",
    "\n",
    "Выберете несколько не похожих по смыслу слов (не таких как в семинаре), и протестируйте полученные эмбединги (найдите ближайшие слова и оцените качество, как в семинаре). \n",
    "Постарайтесь обучать модели как можно дольше и на как можно большем количестве данных. (Но если у вас мало времени или ресурсов, то допустимо взять поменьше данных и поставить меньше эпох)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cde5fd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/akatsnelson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import zipfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pymorphy2\n",
    "nltk.download('punkt')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import chain\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde76f98-4444-4656-84c3-4eb7a347f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/yutkin/Lenta.Ru-News-Dataset/releases/download/v1.1/lenta-ru-news.csv.bz2\"\n",
    "file_name = \"lenta-ru-news.csv.bz2\"\n",
    "if not os.path.exists(file_name):\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        r = requests.get(url)\n",
    "        f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f71d7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pm/xshykg4d7wvgy969hfm8hgp40000gn/T/ipykernel_11219/2111616842.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  lenta_df = pd.read_csv(file_name, compression='bz2')\n",
      "100%|█████████████████████████████████████████| 500/500 [00:07<00:00, 68.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано 499 текстов.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lenta_df = pd.read_csv(file_name, compression='bz2')\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    tokens = re.findall('[а-яё]+', text)\n",
    "    lemmas = [morph.parse(token)[0].normal_form for token in tokens]\n",
    "    lemmas = [lemma for lemma in lemmas if len(lemma) > 2]\n",
    "    return lemmas\n",
    "\n",
    "lenta_df_sample = lenta_df.sample(500, random_state=42) # долго джать больше извните(\n",
    "texts = [preprocess_text(text) for text in tqdm(lenta_df_sample['text'])]\n",
    "texts = [text for text in texts if text]\n",
    "print(f\"Обработано {len(texts)} текстов.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdadad15-297a-44ab-938e-ca031120d622",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Генерация обучающих пар: 100%|███████████████| 499/499 [00:03<00:00, 127.15it/s]\n"
     ]
    }
   ],
   "source": [
    "word_counts = Counter(chain.from_iterable(texts))\n",
    "vocab = [word for word, count in word_counts.items() if count >= 5]\n",
    "\n",
    "word_to_ind = {word: i for i, word in enumerate(vocab)}\n",
    "ind_to_word = {i: w for w, i in word_to_ind.items()}\n",
    "\n",
    "indexed_texts = [\n",
    "    [word_to_ind[word] for word in text if word in word_to_ind]\n",
    "    for text in texts\n",
    "]\n",
    "indexed_texts = [text for text in indexed_texts if len(text) > 1]\n",
    "\n",
    "word_freqs = np.array([word_counts[word] for word in vocab])\n",
    "word_freqs_powered = word_freqs ** 0.75\n",
    "unigram_dist = torch.from_numpy(word_freqs_powered / word_freqs_powered.sum()).float()\n",
    "\n",
    "def get_training_data(indexed_texts, window_size, num_negative_samples):\n",
    "    center_words = []\n",
    "    context_words = []\n",
    "    negative_words = []\n",
    "\n",
    "    unigram_probs = unigram_dist\n",
    "\n",
    "    for text in tqdm(indexed_texts, desc=\"Генерация обучающих пар\"):\n",
    "        text_len = len(text)\n",
    "        for i in range(text_len):\n",
    "            window_start = max(0, i - window_size)\n",
    "            window_end = min(text_len, i + window_size + 1)\n",
    "            center = text[i]\n",
    "\n",
    "            for j in range(window_start, window_end):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                context = text[j]\n",
    "\n",
    "                center_words.append(center)\n",
    "                context_words.append(context)\n",
    "\n",
    "                neg_samples = torch.multinomial(unigram_probs, num_negative_samples, replacement=True)\n",
    "                negative_words.append(neg_samples)\n",
    "\n",
    "    return (torch.tensor(center_words, dtype=torch.long),\n",
    "            torch.tensor(context_words, dtype=torch.long),\n",
    "            torch.stack(negative_words))\n",
    "\n",
    "WINDOW_SIZE = 3\n",
    "NUM_NEGATIVE_SAMPLES = 5\n",
    "\n",
    "center_words, pos_context, neg_context = get_training_data(indexed_texts, WINDOW_SIZE, NUM_NEGATIVE_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fb02cfb-96c4-43f4-9ed4-1574f6fd6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, center, pos, neg):\n",
    "        self.center = center\n",
    "        self.pos = pos\n",
    "        self.neg = neg\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.center)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.center[idx], self.pos[idx], self.neg[idx]\n",
    "\n",
    "class SkipGramNegativeSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.in_embed.weight.data.uniform_(-1, 1)\n",
    "        self.out_embed.weight.data.uniform_(-1, 1)\n",
    "\n",
    "    def forward(self, center_words, context_words, negative_words):\n",
    "        center_emb = self.in_embed(center_words) \n",
    "        context_emb = self.out_embed(context_words) \n",
    "        negative_emb = self.out_embed(negative_words)\n",
    "\n",
    "        log_pos = torch.sum(torch.mul(center_emb, context_emb), dim=1)\n",
    "        neg_scores = torch.bmm(negative_emb, center_emb.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        log_pos = F.logsigmoid(log_pos)\n",
    "        log_neg = F.logsigmoid(-neg_scores).sum(1)\n",
    "        \n",
    "        loss = -(log_pos + log_neg).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09faa27a-62be-48ec-ad2d-5747bd8ac070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 1/5: 100%|█████████████████████████████| 328/328 [00:01<00:00, 173.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, Средняя ошибка: 8.4296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 2/5: 100%|█████████████████████████████| 328/328 [00:01<00:00, 170.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 2, Средняя ошибка: 7.2239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 3/5: 100%|█████████████████████████████| 328/328 [00:02<00:00, 161.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 3, Средняя ошибка: 6.1883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 4/5: 100%|█████████████████████████████| 328/328 [00:01<00:00, 170.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 4, Средняя ошибка: 5.1885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Эпоха 5/5: 100%|█████████████████████████████| 328/328 [00:01<00:00, 181.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 5, Средняя ошибка: 4.3069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 5 \n",
    "LEARNING_RATE = 0.001\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "skipgram_model = SkipGramNegativeSampling(VOCAB_SIZE, EMBEDDING_DIM).to(device)\n",
    "dataset = Word2VecDataset(center_words, pos_context, neg_context)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = optim.Adam(skipgram_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for i, (center, pos, neg) in enumerate(tqdm(dataloader, desc=f\"Эпоха {epoch+1}/{EPOCHS}\")):\n",
    "        center, pos, neg = center.to(device), pos.to(device), neg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = skipgram_model(center, pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Эпоха {epoch+1}, Средняя ошибка: {total_loss / len(dataloader):.4f}\")\n",
    "\n",
    "skipgram_embeddings = skipgram_model.in_embed.weight.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e86b20d5-5df1-4801-bdf7-227d33bf399e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ближайшие слова для 'россия':\n",
      "  - командир (сходство: 0.4272)\n",
      "  - поставка (сходство: 0.4255)\n",
      "  - процент (сходство: 0.4150)\n",
      "  - решение (сходство: 0.4036)\n",
      "  - они (сходство: 0.3939)\n",
      "  - счёт (сходство: 0.3819)\n",
      "  - подписать (сходство: 0.3685)\n",
      "  - центральный (сходство: 0.3650)\n",
      "  - кпрф (сходство: 0.3638)\n",
      "  - момент (сходство: 0.3637)\n",
      "--------------------\n",
      "Ближайшие слова для 'нефть':\n",
      "  - инвестиционный (сходство: 0.3618)\n",
      "  - секретарь (сходство: 0.3617)\n",
      "  - жилец (сходство: 0.3585)\n",
      "  - матч (сходство: 0.3550)\n",
      "  - эффективность (сходство: 0.3485)\n",
      "  - принятие (сходство: 0.3469)\n",
      "  - ленинградский (сходство: 0.3345)\n",
      "  - доступно (сходство: 0.3336)\n",
      "  - медведев (сходство: 0.3159)\n",
      "  - министр (сходство: 0.3125)\n",
      "--------------------\n",
      "Слово 'любовь' не найдено в словаре.\n",
      "--------------------\n",
      "Ближайшие слова для 'компьютер':\n",
      "  - специалист (сходство: 0.4346)\n",
      "  - пресс (сходство: 0.3612)\n",
      "  - коррупция (сходство: 0.3498)\n",
      "  - предыдущий (сходство: 0.3456)\n",
      "  - парламентский (сходство: 0.3399)\n",
      "  - валенса (сходство: 0.3385)\n",
      "  - обеспечивать (сходство: 0.3336)\n",
      "  - минск (сходство: 0.3283)\n",
      "  - момент (сходство: 0.3278)\n",
      "  - видеть (сходство: 0.3220)\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "def find_nearest(word, embeddings, top_n=10):\n",
    "    if word not in word_to_ind:\n",
    "        print(f\"Слово '{word}' не найдено в словаре.\")\n",
    "        return\n",
    "\n",
    "    word_idx = word_to_ind[word]\n",
    "    word_vec = embeddings[word_idx]\n",
    "\n",
    "    similarities = cosine_similarity([word_vec], embeddings)[0]\n",
    "\n",
    "    nearest_indices = similarities.argsort()[-top_n-1:-1][::-1]\n",
    "\n",
    "    print(f\"Ближайшие слова для '{word}':\")\n",
    "    for idx in nearest_indices:\n",
    "        print(f\"  - {ind_to_word[idx]} (сходство: {similarities[idx]:.4f})\")\n",
    "\n",
    "test_words = ['россия', 'нефть', 'любовь', 'компьютер']\n",
    "for word in test_words:\n",
    "    find_nearest(word, skipgram_embeddings)\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b61b7c",
   "metadata": {},
   "source": [
    "# Задание 2 (2 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66eff080",
   "metadata": {},
   "source": [
    "Обучите 1 word2vec и 1 fastext модель в gensim. В каждой из модели нужно задать все параметры, которые мы разбирали на семинаре. Заданные значения должны отличаться от дефолтных и от тех, что мы использовали на семинаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5baa094-bcdb-468c-bb9c-6fdaa8055c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "w2v_model_gensim = Word2Vec(\n",
    "    sentences=texts,    \n",
    "    vector_size=150,\n",
    "    window=8,\n",
    "    min_count=10,\n",
    "    sg=1,\n",
    "    hs=0,\n",
    "    negative=15,\n",
    "    epochs=10,\n",
    "    workers=4 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d244217c-e26b-4d9c-9fa1-1b0acd60a44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model_gensim = FastText(\n",
    "    sentences=texts,\n",
    "    vector_size=200,\n",
    "    window=5,\n",
    "    min_count=8,\n",
    "    sg=1,\n",
    "    epochs=10,\n",
    "    min_n=3,\n",
    "    max_n=6,\n",
    "    workers=4\n",
    ")\n",
    "ft_model_gensim.max_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "986c2018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ближайшие к 'россия': [('крым', 0.8023478388786316), ('корея', 0.7896313071250916), ('путин', 0.7829669117927551), ('владимир', 0.7795822620391846), ('казахстан', 0.7745606303215027)]\n",
      "Ближайшие к 'нефть': [('нефтяной', 0.8904008865356445), ('добыча', 0.8843613862991333), ('поставка', 0.8790572881698608), ('завод', 0.8719830513000488), ('газпром', 0.8709684014320374)]\n",
      "Слово 'любовь' не найдено в модели Word2Vec.\n",
      "Ближайшие к 'компьютер': [('телефон', 0.9519345760345459), ('смартфон', 0.9435363411903381), ('пользователь', 0.9263903498649597), ('способный', 0.9257462024688721), ('диск', 0.9249487519264221)]\n",
      "Ближайшие к 'россия': [('кандидат', 0.9551265835762024), ('партия', 0.9515881538391113), ('миссия', 0.9464306831359863), ('белоруссия', 0.9422648549079895), ('российский', 0.9307098984718323)]\n",
      "Ближайшие к 'нефть': [('роснефть', 0.9827443361282349), ('долг', 0.979930579662323), ('доля', 0.9720900654792786), ('доллар', 0.9602851271629333), ('нефтяной', 0.9593896865844727)]\n",
      "Ближайшие к 'любовь': [('спецназ', 0.9893226027488708), ('фирма', 0.9884377717971802), ('оператор', 0.9878275394439697), ('любой', 0.9870924353599548), ('сфера', 0.9869356751441956)]\n",
      "Ближайшие к 'компьютер': [('карта', 0.9869505167007446), ('морской', 0.9845619201660156), ('орбита', 0.9829700589179993), ('телескоп', 0.9792827367782593), ('яндекс', 0.9790414571762085)]\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    if word in w2v_model_gensim.wv:\n",
    "        print(f\"Ближайшие к '{word}': {w2v_model_gensim.wv.most_similar(word, topn=5)}\")\n",
    "    else:\n",
    "        print(f\"Слово '{word}' не найдено в модели Word2Vec.\")\n",
    "\n",
    "for word in test_words:\n",
    "    if word in ft_model_gensim.wv:\n",
    "        print(f\"Ближайшие к '{word}': {ft_model_gensim.wv.most_similar(word, topn=5)}\")\n",
    "    else:\n",
    "        print(f\"Слово '{word}' не найдено в словаре FastText, но вектор можно получить.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bb928c",
   "metadata": {},
   "source": [
    "# Задание 3 (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3019b0d1",
   "metadata": {},
   "source": [
    "Используя датасет для классификации (labeled.csv), обучите классификатор на базе эмбеддингов. Оцените качество на отложенной выборке.   \n",
    "В качестве эмбеддинг модели вы можете использовать одну из моделей обученных в предыдущем задании или использовать одну из предобученных моделей с rusvectores (удостоверьтесь что правильно воспроизводите предобработку в этом случае!)  \n",
    "Для того, чтобы построить эмбединг целого текста, усредните вектора отдельных слов в один общий вектор. \n",
    "В качестве алгоритма классификации используйте LogisicticRegression (можете попробовать SGDClassifier, чтобы было побыстрее)  \n",
    "F1 мера должна быть выше 20%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "666bf9e8-be83-4792-bab0-97832d3b90aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_labeled = \"https://github.com/mannefedov/compling_nlp_hse_course/raw/refs/heads/master/notebooks/word_embeddings/labeled.csv\"\n",
    "file_labeled = \"labeled.csv\"\n",
    "if not os.path.exists(file_labeled):\n",
    "    r = requests.get(url_labeled)\n",
    "    with open(file_labeled, 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed908832",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_class = pd.read_csv(file_labeled)\n",
    "df_class['toxic'] = (df_class['toxic'] > 0).astype(int)\n",
    "\n",
    "df_class['lemmas'] = df_class['comment'].apply(preprocess_text)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_class['lemmas'],\n",
    "    df_class['toxic'],\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df_class['toxic']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f0accb1-0e32-44eb-9207-fa48ea271274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация текстов\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 11529/11529 [00:01<00:00, 7180.98it/s]\n",
      "100%|█████████████████████████████████████| 2883/2883 [00:00<00:00, 7032.41it/s]\n"
     ]
    }
   ],
   "source": [
    "def text_to_vector(text_lemmas, model):\n",
    "    word_vectors = []\n",
    "    for word in text_lemmas:\n",
    "        if word in model.wv:\n",
    "            word_vectors.append(model.wv[word])\n",
    "\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "embedding_model = ft_model_gensim\n",
    "\n",
    "print(\"Векторизация текстов\")\n",
    "X_train_vec = np.array([text_to_vector(text, embedding_model) for text in tqdm(X_train)])\n",
    "X_test_vec = np.array([text_to_vector(text, embedding_model) for text in tqdm(X_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55818dff-4bff-4887-9727-951a8d740484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1-мера (weighted) на тестовой выборке: 0.7418\n",
      "Успех! F1-мера выше 20%.\n",
      "\n",
      "Отчет по классификации:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.91      0.83      1918\n",
      "           1       0.72      0.46      0.56       965\n",
      "\n",
      "    accuracy                           0.76      2883\n",
      "   macro avg       0.74      0.68      0.70      2883\n",
      "weighted avg       0.75      0.76      0.74      2883\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "log_reg.fit(X_train_vec, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test_vec)\n",
    "\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "print(f\"\\nF1-мера (weighted) на тестовой выборке: {f1:.4f}\")\n",
    "\n",
    "if f1 > 0.20:\n",
    "    print(\"Успех! F1-мера выше 20%.\")\n",
    "else:\n",
    "    print(\"F1-мера ниже 20%.\")\n",
    "\n",
    "print(\"\\nОтчет по классификации:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c18c5a",
   "metadata": {},
   "source": [
    "# Задание 4 (2 доп балла)\n",
    "\n",
    "В тетрадку с фастекстом добавьте код для обучения с negative sampling (задача сводится к бинарной классификации) и обучите модель. Проверьте полученную модель на нескольких словах. Похожие слова должны быть похожими по смыслу и по форме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d437a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер основного словаря: 2744\n",
      "Размер полного словаря (с n-граммами): 32695\n"
     ]
    }
   ],
   "source": [
    "def get_subwords(word, min_n=3, max_n=6):\n",
    "    \"\"\"Создает n-граммы для слова.\"\"\"\n",
    "    subwords = []\n",
    "    word = '<' + word + '>'\n",
    "    for i in range(len(word)):\n",
    "        for j in range(i + min_n, min(i + max_n + 1, len(word) + 1)):\n",
    "            subwords.append(word[i:j])\n",
    "    return subwords\n",
    "\n",
    "subword_vocab = set()\n",
    "for word in vocab:\n",
    "    subwords = get_subwords(word, min_n=3, max_n=6)\n",
    "    for sub in subwords:\n",
    "        subword_vocab.add(sub)\n",
    "\n",
    "\n",
    "full_vocab = vocab + list(subword_vocab)\n",
    "print(f\"Размер основного словаря: {len(vocab)}\")\n",
    "print(f\"Размер полного словаря (с n-граммами): {len(full_vocab)}\")\n",
    "\n",
    "\n",
    "word_to_ind_ft = {word: i for i, word in enumerate(full_vocab)}\n",
    "ind_to_word_ft = {i: word for i, word in enumerate(full_vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edcfa0b2-a35f-4c03-823b-800ba79eee47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FastTextNegativeSampling(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.out_embed = nn.Embedding(len(vocab), embedding_dim)\n",
    "        self.in_embed.weight.data.uniform_(-1, 1)\n",
    "        self.out_embed.weight.data.uniform_(-1, 1)\n",
    "\n",
    "    def get_word_vector(self, word_idx_with_subwords):\n",
    "        word_emb = self.in_embed(word_idx_with_subwords)\n",
    "        return torch.mean(word_emb, dim=0)\n",
    "\n",
    "    def forward(self, center_indices, context_indices, negative_indices):\n",
    "        center_emb_list = []\n",
    "        for word_subwords in center_indices:\n",
    "            word_subwords_tensor = torch.LongTensor(word_subwords).to(device)\n",
    "            center_emb_list.append(self.get_word_vector(word_subwords_tensor))\n",
    "\n",
    "        center_emb = torch.stack(center_emb_list) \n",
    "\n",
    "        context_emb = self.out_embed(context_indices)\n",
    "        negative_emb = self.out_embed(negative_indices)\n",
    "\n",
    "        log_pos = torch.sum(torch.mul(center_emb, context_emb), dim=1)\n",
    "        log_pos = F.logsigmoid(log_pos)\n",
    "\n",
    "        neg_scores = torch.bmm(negative_emb, center_emb.unsqueeze(2)).squeeze(2)\n",
    "        log_neg = F.logsigmoid(-neg_scores).sum(1)\n",
    "\n",
    "        loss = -(log_pos + log_neg).mean()\n",
    "        return loss\n",
    "\n",
    "def get_indices_with_subwords(word_idx):\n",
    "    word = ind_to_word[word_idx]\n",
    "    subwords = get_subwords(word)\n",
    "    indices = [word_to_ind_ft.get(w, -1) for w in [word] + subwords]\n",
    "    return [i for i in indices if i != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57deb8d8-24f6-43a9-93ea-1e6cedc19c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 10000/10000 [00:42<00:00, 237.75it/s]\n"
     ]
    }
   ],
   "source": [
    "ft_model_scratch = FastTextNegativeSampling(len(full_vocab), EMBEDDING_DIM).to(device)\n",
    "optimizer_ft = optim.Adam(ft_model_scratch.parameters(), lr=0.001)\n",
    "\n",
    "num_samples_to_train = 10000\n",
    "train_subset_indices = np.random.choice(len(center_words), num_samples_to_train)\n",
    "\n",
    "ft_model_scratch.train()\n",
    "for i in tqdm(range(num_samples_to_train)):\n",
    "    idx = train_subset_indices[i]\n",
    "    center_idx = center_words[idx].item()\n",
    "    context_idx = pos_context[idx].unsqueeze(0).to(device) # batch_size=1\n",
    "    neg_idx = neg_context[idx].unsqueeze(0).to(device) # batch_size=1\n",
    "\n",
    "\n",
    "    center_with_subwords_indices = [get_indices_with_subwords(center_idx)]\n",
    "\n",
    "    optimizer_ft.zero_grad()\n",
    "    loss = ft_model_scratch(center_with_subwords_indices, context_idx, neg_idx)\n",
    "    loss.backward()\n",
    "    optimizer_ft.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ec7889c-88db-40b7-8364-02b29ef74d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ft_vector_scratch(word, model):\n",
    "    subwords = get_subwords(word)\n",
    "    indices = [word_to_ind_ft.get(w, -1) for w in [word] + subwords]\n",
    "    indices = [i for i in indices if i != -1]\n",
    "    if not indices:\n",
    "        return np.zeros(model.in_embed.embedding_dim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        indices_tensor = torch.LongTensor(indices).to(device)\n",
    "        vector = model.in_embed(indices_tensor).mean(dim=0).cpu().numpy()\n",
    "    return vector\n",
    "\n",
    "def find_nearest_ft_scratch(word, model, top_n=10):\n",
    "    word_vec = get_ft_vector_scratch(word, model)\n",
    "\n",
    "    all_word_vectors = []\n",
    "    for i in range(len(vocab)):\n",
    "        v = get_ft_vector_scratch(ind_to_word[i], model)\n",
    "        all_word_vectors.append(v)\n",
    "    all_word_vectors = np.array(all_word_vectors)\n",
    "\n",
    "    similarities = cosine_similarity([word_vec], all_word_vectors)[0]\n",
    "    nearest_indices = similarities.argsort()[-top_n-1:-1][::-1]\n",
    "\n",
    "    print(f\"Ближайшие слова для '{word}' (FastText с нуля):\")\n",
    "    for idx in nearest_indices:\n",
    "        print(f\"  - {ind_to_word[idx]} (сходство: {similarities[idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bd2f2d4-83c1-4b52-837e-9ceccdb68d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ближайшие слова для 'учитель' (FastText с нуля):\n",
      "  - житель (сходство: 0.4780)\n",
      "  - руководитель (сходство: 0.4626)\n",
      "  - водитель (сходство: 0.4352)\n",
      "  - представитель (сходство: 0.4197)\n",
      "  - строитель (сходство: 0.4153)\n",
      "  - посетитель (сходство: 0.4141)\n",
      "  - исполнитель (сходство: 0.3998)\n",
      "  - строительство (сходство: 0.3966)\n",
      "  - значительный (сходство: 0.3896)\n",
      "  - заместитель (сходство: 0.3815)\n",
      "--------------------\n",
      "Ближайшие слова для 'крым' (FastText с нуля):\n",
      "  - приём (сходство: 0.3588)\n",
      "  - миссия (сходство: 0.3259)\n",
      "  - выбор (сходство: 0.3188)\n",
      "  - донбасс (сходство: 0.3160)\n",
      "  - ребёнок (сходство: 0.2883)\n",
      "  - минимум (сходство: 0.2684)\n",
      "  - уволить (сходство: 0.2663)\n",
      "  - благодаря (сходство: 0.2659)\n",
      "  - форма (сходство: 0.2543)\n",
      "  - информация (сходство: 0.2468)\n"
     ]
    }
   ],
   "source": [
    "ft_model_scratch.eval()\n",
    "\n",
    "find_nearest_ft_scratch('учитель', ft_model_scratch)\n",
    "print(\"-\" * 20)\n",
    "find_nearest_ft_scratch('крым', ft_model_scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592ab4d-b32f-43c2-82ba-3dcc6f6b2c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
