{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371970ff",
   "metadata": {},
   "source": [
    "# Домашнее задание № 3. Исправление опечаток"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35cf8bd",
   "metadata": {},
   "source": [
    "## 1. Доп. ранжирование по вероятности (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6be25c",
   "metadata": {},
   "source": [
    "Дополните get_closest_hybrid_match в семинаре так, чтобы из кандадатов с одинаковым расстоянием редактирования выбиралось наиболее вероятное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "485fcc42-c3a0-4b9b-ae65-3ad80af467a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textdistance\n",
      "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
      "Downloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
      "Installing collected packages: textdistance\n",
      "Successfully installed textdistance-4.6.3\n"
     ]
    }
   ],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2659e1e8-ac0c-4052-82b7-839c362319d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-06-29 21:41:30--  https://github.com/mannefedov/compling_nlp_hse_course/raw/refs/heads/master/notebooks/spelling/data.zip\n",
      "Resolving github.com (github.com)... 140.82.121.4\n",
      "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/refs/heads/master/notebooks/spelling/data.zip [following]\n",
      "--2025-06-29 21:41:31--  https://raw.githubusercontent.com/mannefedov/compling_nlp_hse_course/refs/heads/master/notebooks/spelling/data.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 18787000 (18M) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>]  17.92M  5.59MB/s    in 3.4s    \n",
      "\n",
      "2025-06-29 21:41:36 (5.22 MB/s) - ‘data.zip’ saved [18787000/18787000]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/mannefedov/compling_nlp_hse_course/raw/refs/heads/master/notebooks/spelling/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a620dca8-690f-4c35-9071-59be10403508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "  inflating: correct_sents.txt       \n",
      "  inflating: sents_with_mistakes.txt  \n",
      "  inflating: wiki_data.txt           \n",
      "  inflating: __MACOSX/._wiki_data.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e67f8d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import textdistance\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3649d46-f85c-48bc-8f96-9af30a50a8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_PATH = './sents_with_mistakes.txt'\n",
    "TRUE_PATH = './correct_sents.txt'\n",
    "WIKI_PATH = './wiki_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1ebee19-f777-4e2a-9102-8f65fdb16353",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_sents = open(BAD_PATH, encoding='utf8').read().splitlines()\n",
    "true_sents = open(TRUE_PATH, encoding='utf8').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea4f83a8-5ec1-4bfc-a4e8-4ddff9664612",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT_ALL = punctuation + \"«»—…“”\"\n",
    "\n",
    "def align_words(sent1: str, sent2: str):\n",
    "    t1 = [tok.strip(PUNCT_ALL) for tok in sent1.lower().split() if tok.strip(PUNCT_ALL)]\n",
    "    t2 = [tok.strip(PUNCT_ALL) for tok in sent2.lower().split() if tok.strip(PUNCT_ALL)]\n",
    "    return list(zip(t1, t2))\n",
    "\n",
    "mistake_pairs = []\n",
    "for tr, bd in zip(true_sents, bad_sents):\n",
    "    for w_true, w_bad in align_words(tr, bd):\n",
    "        mistake_pairs.append((w_true, w_bad))\n",
    "\n",
    "SAMPLE_SIZE = 5000\n",
    "random.seed(42)\n",
    "eval_pairs = random.sample(mistake_pairs, min(SAMPLE_SIZE, len(mistake_pairs)))\n",
    "\n",
    "wiki_text = open(WIKI_PATH, encoding='utf8').read().lower()\n",
    "vocab = Counter(re.findall(r\"\\w+\", wiki_text))\n",
    "N = sum(vocab.values())\n",
    "\n",
    "def P(word: str) -> float:\n",
    "    return vocab[word] / N if word in vocab else 0.0\n",
    "\n",
    "def is_mistaken(word: str) -> bool:\n",
    "    return word not in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf8603d3-4963-48ce-b42f-ab2699729e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = list(vocab.keys())\n",
    "vectorizer = CountVectorizer(analyzer='char', ngram_range=(1,3), max_features=10000)\n",
    "X = vectorizer.fit_transform(word_list)\n",
    "id2word = {i: word_list[i] for i in range(len(word_list))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c11c01cd-80b5-42ed-9c66-ab937d16efa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_vec(text: str, topn: int = 20):\n",
    "    v = vectorizer.transform([text])\n",
    "    dists = cosine_distances(v, X)[0]\n",
    "    idx = np.argsort(dists)[:topn]\n",
    "    return [(id2word[i], dists[i]) for i in idx]\n",
    "\n",
    "def get_closest_hybrid_match(text: str, topn: int = 3, max_vec: int = None):\n",
    "    if max_vec is None:\n",
    "        max_vec = topn * 4\n",
    "    vec_cands = get_closest_vec(text, topn=max_vec)\n",
    "    scored = []\n",
    "    for w, _ in vec_cands:\n",
    "        dist = textdistance.damerau_levenshtein.distance(text, w)\n",
    "        prob = P(w)\n",
    "        scored.append((w, dist, prob))\n",
    "    scored.sort(key=lambda x: (x[1], -x[2]))\n",
    "    return [(w, d) for w, d, p in scored[:topn]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "983b9df6-1bc3-465f-bea3-12edaa55a089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 5000/5000 [03:30<00:00, 23.78it/s]\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "mistakes_total = 0\n",
    "mistakes_fixed = 0\n",
    "correct_total = 0\n",
    "correct_broken = 0\n",
    "\n",
    "for w_true, w_bad in tqdm(eval_pairs):\n",
    "    if is_mistaken(w_bad):\n",
    "        pred = get_closest_hybrid_match(w_bad, topn=1)[0][0]\n",
    "    else:\n",
    "        pred = w_bad\n",
    "    total += 1\n",
    "    if pred == w_true:\n",
    "        correct += 1\n",
    "    if w_true != w_bad:\n",
    "        mistakes_total += 1\n",
    "        if pred == w_true:\n",
    "            mistakes_fixed += 1\n",
    "    else:\n",
    "        correct_total += 1\n",
    "        if pred != w_true:\n",
    "            correct_broken += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9718652a-fb3b-4fd2-8cc4-2811783127c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Гибридный метод с вероятностью\n",
      "Точность: 0.8568\n",
      "Процент исправленных ошибок: 0.4554\n",
      "Процент испорченных корректных слов: 0.0844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Гибридный метод с вероятностью\")\n",
    "print(f\"Точность: {correct/total:.4f}\")\n",
    "print(f\"Процент исправленных ошибок: {mistakes_fixed/mistakes_total:.4f}\")\n",
    "print(f\"Процент испорченных корректных слов: {correct_broken/correct_total:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf9985",
   "metadata": {},
   "source": [
    "## 2.  Symspell (7 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9392cc23",
   "metadata": {},
   "source": [
    "Реализуйте алгоритм Symspell. Он похож на алгоритм Норвига, но проще и быстрее. Он основан только на одной операции - удалении символа. Описание алгоритма по шагам:\n",
    "\n",
    "1) Составляется словарь правильных слов  \n",
    "2) На основе словаря правильных слов составляется словарь удалений - для каждого правильного слова создаются все варианты удалений и создается словарь, где ключ - слово с удалением, а значение - правильное слово  (!) \n",
    "3) Для выбора исправления для слова с опечаткой генерируются все варианты удаления, из них выбираются те, что есть в словаре удалений, построенного на шаге 2. Слово с опечаткой заменяется на правильное слово, соответствующее варианту удаления  \n",
    "4) Если в словаре удалений есть несколько вариантов, то выбирается удаление, которому соответствует наиболее вероятное правильное слово  \n",
    "\n",
    "\n",
    "Оцените качество полученного алгоритма теми же тремя метриками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a298614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_deletes(word: str, max_edit: int = 2):\n",
    "    deletes = set()\n",
    "    queue = [word]\n",
    "    for d in range(max_edit):\n",
    "        next_queue = []\n",
    "        for w in queue:\n",
    "            for i in range(len(w)):\n",
    "                var = w[:i] + w[i+1:]\n",
    "                if var not in deletes:\n",
    "                    deletes.add(var)\n",
    "                    next_queue.append(var)\n",
    "        queue = next_queue\n",
    "    return deletes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0eca040-0472-4dac-b8fd-9cd0e685d2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EDIT = 2\n",
    "delete_dict = defaultdict(set)\n",
    "for w in vocab:\n",
    "    for d in generate_deletes(w, MAX_EDIT):\n",
    "        delete_dict[d].add(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbe8a982-87b2-4661-bc30-e90bee12e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symspell_correct(word: str, max_edit: int = MAX_EDIT) -> str:\n",
    "    if word in vocab:\n",
    "        return word\n",
    "    cands = set()\n",
    "    for d in generate_deletes(word, max_edit):\n",
    "        if d in delete_dict:\n",
    "            cands.update(delete_dict[d])\n",
    "    if not cands:\n",
    "        return word\n",
    "    # фильтруем по реальному расстоянию\n",
    "    scored = []\n",
    "    for w in cands:\n",
    "        dist = textdistance.damerau_levenshtein.distance(word, w)\n",
    "        if dist <= max_edit:\n",
    "            scored.append((w, dist, P(w)))\n",
    "    if not scored:\n",
    "        return word\n",
    "    scored.sort(key=lambda x: (x[1], -x[2]))\n",
    "    return scored[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2276870-e25c-4352-8e18-6f5819372219",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 5000/5000 [00:00<00:00, 15515.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SymSpell\n",
      "Точность: 0.8726\n",
      "Процент исправленных ошибок: 0.4726\n",
      "Процент испорченных корректных слов: 0.0688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "correct = 0\n",
    "mistakes_total = 0\n",
    "mistakes_fixed = 0\n",
    "correct_total = 0\n",
    "correct_broken = 0\n",
    "\n",
    "for w_true, w_bad in tqdm(eval_pairs):\n",
    "    if is_mistaken(w_bad):\n",
    "        pred = symspell_correct(w_bad)\n",
    "    else:\n",
    "        pred = w_bad\n",
    "    total += 1\n",
    "    if pred == w_true:\n",
    "        correct += 1\n",
    "    if w_true != w_bad:\n",
    "        mistakes_total += 1\n",
    "        if pred == w_true:\n",
    "            mistakes_fixed += 1\n",
    "    else:\n",
    "        correct_total += 1\n",
    "        if pred != w_true:\n",
    "            correct_broken += 1\n",
    "\n",
    "print(\"SymSpell\")\n",
    "print(f\"Точность: {correct/total:.4f}\")\n",
    "print(f\"Процент исправленных ошибок: {mistakes_fixed/mistakes_total:.4f}\")\n",
    "print(f\"Процент испорченных корректных слов: {correct_broken/correct_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a31242c-f0d7-4760-be9c-f88c1091aece",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
